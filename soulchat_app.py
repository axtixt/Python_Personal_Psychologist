
''' è¿è¡Œæ–¹å¼
```bash
pip install streamlit # ç¬¬ä¸€æ¬¡è¿è¡Œéœ€è¦å®‰è£…streamlit
pip install streamlit_chat # ç¬¬ä¸€æ¬¡è¿è¡Œéœ€è¦å®‰è£…streamlit_chat
streamlit run soulchat_app.py --server.port 9026
```
## æµ‹è¯•è®¿é—®
http://<your_ip>:9026

'''

import os
import re
import json
import torch
import streamlit as st
from streamlit_chat import message
import pyttsx3  # ä½¿ç”¨è·¨å¹³å°çš„ç³»ç»Ÿçº§TTS

st.set_page_config(
    page_title="å¿ƒéˆå°å¹«æ‰‹",
    page_icon="ğŸ‘©â€ğŸ«",
    layout="wide",
    initial_sidebar_state="expanded",
    menu_items={
        'About': """     
-   ç‰ˆæœ¬ï¼šå¿ƒéˆå°å¹«æ‰‹
-   ä½œè€…ï¼šå³ä¸–æ°
        """
    }
)

from transformers import AutoModel, AutoTokenizer
from igbug import IG_Parser
from xbug import TwitterExtractor
from datetime import datetime
from config import TWITTER_AUTH_TOKEN
from hanlp_restful import HanLPClient
from opencc import OpenCC
import threading
import tempfile
import pygame
from gtts import gTTS
import requests
import pandas as pd
import plotly.express as px

# st-chat uses https://www.dicebear.com/styles for the avatar
# https://emoji6.com/emojiall/
model_name_or_path = 'scutcyr/SoulChat'
# æŒ‡å®šæ˜¾å¡è¿›è¡Œæ¨ç†
os.environ['CUDA_VISIBLE_DEVICES'] = '0' # é»˜è®¤ä½¿ç”¨0å·æ˜¾å¡ï¼Œé¿å…Windowsç”¨æˆ·å¿˜è®°ä¿®æ”¹è¯¥å¤„
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)

HanLP = HanLPClient('https://www.hanlp.com/api', auth='ODQ4OEBiYnMuaGFubHAuY29tOmt6WnlQaDRTZ0N6RUt5OGI=', language='zh')

sensitive_keywords = ['æ®º', 'æ­»', 'è‡ªæ®º', 'ä¸æƒ³æ´»äº†', 'æƒ³æ­»', 'æ´»ä¸ä¸‹å»', 'çµæŸç”Ÿå‘½', 'é›¢é–‹äººä¸–']
friendship_keywords = ['æƒ³è­˜æœ‹å‹', 'èªè­˜æœ‹å‹', 'äº¤æœ‹å‹', 'çµè­˜æœ‹å‹', 'äº¤å‹', 'æƒ³è­˜æ–°æœ‹å‹', 'èªè­˜æ–°äº¤å‹' , 'æƒ³è­˜æ–°æœ‹å‹'] 

@st.cache_resource
def load_model():
    model = AutoModel.from_pretrained(model_name_or_path, trust_remote_code=True).half()
    model.to(device)
    print('Model Load done!')
    return model

@st.cache_resource
def load_tokenizer():
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=True)
    print('Tokenizer Load done!')
    return tokenizer

model = load_model()
tokenizer = load_tokenizer()

class InstagramDataFetcher:
    def __init__(self, account: str, download_num: int):
        self.account = account
        self.download_num = download_num
        self.saved_contents = []
        self.post_idx = 0

    def fetch_data(self):
        try:
            parser = IG_Parser(self.account)
            # ç›´æ¥è·å–è§£æåçš„æ•°æ®
            self.saved_contents = parser.start_parse(self.download_num)
            
            # æ·»åŠ ç»Ÿä¸€æ•°æ®ç»“æ„
            unified_data = []
            for post in self.saved_contents:
                # å¤„ç†æ—¥æœŸæ ¼å¼ - ç°åœ¨æ—¥æœŸå­—æ®µåä¸º 'datetime'
                post_date = post.get('datetime', '')
                
                # ç¡®ä¿å†…å®¹ä¸ä¸ºç©º - ç°åœ¨å†…å®¹å­—æ®µåä¸º 'text'
                content = post.get('text', '')
                
                unified_data.append({
                    'platform': 'Instagram',
                    'id': post.get('id', ''),
                    'content': content,
                    'url': post.get('url', ''),
                    'date': post_date,
                    'likes': post.get('likes', 0),
                    'comments': post.get('comments', 0)
                })
            return unified_data
        except Exception as e:
            raise Exception(f"Instagramæ•¸æ“šç²å–å¤±æ•—: {str(e)}")

def generate_report(social_data, sentiment_result):
    """æ ¹æ“šæƒ…æ„Ÿæ¨™ç±¤å’Œæ—¥æœŸç”¢ç”Ÿå ±å‘Š"""
    try:
        # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾åˆ°æ¯æ¡æ•°æ®
        for i, item in enumerate(social_data):
            if i < len(sentiment_result):
                # å¤„ç†æƒ…æ„Ÿåˆ†æç»“æœå¯èƒ½æ˜¯æ•°å€¼çš„æƒ…å†µ
                sentiment_value = sentiment_result[i]
                if isinstance(sentiment_value, float):
                    if sentiment_value > 0.6:
                        item['sentiment'] = 'æ­£é¢'
                    elif sentiment_value < 0.4:
                        item['sentiment'] = 'è² é¢'
                    else:
                        item['sentiment'] = 'ä¸­æ€§'
                else:
                    item['sentiment'] = sentiment_value
            else:
                item['sentiment'] = 'æœªçŸ¥'
        
        # åˆ›å»ºæƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
        st.subheader("æƒ…æ„Ÿåˆ†å¸ƒåˆ†æ")
        # æå–æƒ…æ„Ÿæ ‡ç­¾
        sentiment_labels = [item['sentiment'] for item in social_data]
        sentiment_counts = pd.Series(sentiment_labels).value_counts()
            
        fig1 = px.pie(
            sentiment_counts, 
            values=sentiment_counts.values,
            names=sentiment_counts.index,
            title="æƒ…æ„Ÿåˆ†å¸ƒæ¯”ä¾‹",
            color_discrete_sequence=px.colors.qualitative.Pastel
        )
        fig1.update_traces(textposition='inside', textinfo='percent+label')
        st.plotly_chart(fig1, use_container_width=True)
        
        # åˆ›å»ºæ•°æ®æ¡†
        df = pd.DataFrame(social_data)
        
        # ç¡®ä¿æœ‰æ—¥æœŸå­—æ®µ
        if 'date' in df.columns and len(df) > 0:
            # è½¬æ¢æ—¥æœŸæ ¼å¼
            try:
                # ç»Ÿä¸€æ—¥æœŸæ ¼å¼å¤„ç†
                df['formatted_date'] = pd.to_datetime(df['date'], errors='coerce')
                
                # ç§»é™¤æ— æ•ˆæ—¥æœŸ
                df = df.dropna(subset=['formatted_date'])
                
                if not df.empty:
                    # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
                    daily_sentiment = df.groupby([df['formatted_date'].dt.date, 'sentiment']).size().unstack(fill_value=0)
                    
                    # ç¡®ä¿æ‰€æœ‰æƒ…æ„Ÿç±»åˆ«éƒ½å­˜åœ¨
                    all_sentiments = ['æ­£é¢', 'ä¸­æ€§', 'è² é¢']
                    for sentiment in all_sentiments:
                        if sentiment not in daily_sentiment.columns:
                            daily_sentiment[sentiment] = 0
                    
                    # åˆ›å»ºæ¯æ—¥æƒ…æ„Ÿè¶‹åŠ¿å›¾
                    st.subheader("æ¯æ—¥æƒ…æ„Ÿè¶¨å‹¢")
                    fig2 = px.line(
                        daily_sentiment, 
                        x=daily_sentiment.index,
                        y=daily_sentiment.columns,
                        title="æ¯æ—¥æƒ…æ„Ÿè®ŠåŒ–è¶¨å‹¢",
                        labels={'value': 'è²¼æ–‡æ•¸é‡', 'date': 'æ—¥æœŸ'},
                        markers=True
                    )
                    fig2.update_layout(
                        legend_title_text='æƒ…æ„Ÿé¡å‹',
                        xaxis_title='æ—¥æœŸ',
                        yaxis_title='è²¼æ–‡æ•¸é‡'
                    )
                    st.plotly_chart(fig2, use_container_width=True)
                else:
                    st.warning("æ²’æœ‰æœ‰æ•ˆçš„æ—¥æœŸæ•¸æ“š")
            except Exception as e:
                st.error(f"æ—¥æœŸè™•ç†éŒ¯èª¤: {str(e)}")
        
        # æ˜¾ç¤ºæƒ…æ„Ÿæœ€å¼ºçƒˆçš„å¸–å­
        st.subheader("ä»£è¡¨æ€§è²¼æ–‡")
        tab1, tab2, tab3 = st.tabs(["æ­£é¢è²¼æ–‡", "ä¸­æ€§è²¼æ–‡", "è² é¢è²¼æ–‡"])
        
        # åˆ†åˆ«æ˜¾ç¤ºå„ç±»åˆ«ä»£è¡¨æ€§å¸–å­
        with tab1:
            st.markdown("### ğŸ˜Š æœ€å…·ä»£è¡¨æ€§çš„æ­£é¢è²¼æ–‡")
            positive_posts = [post for post in social_data if post.get('sentiment') == 'æ­£é¢']
            if positive_posts:
                # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                positive_posts.sort(key=lambda x: x.get('date', ''), reverse=True)
                for i, post in enumerate(positive_posts[:5]):  # æ˜¾ç¤ºå‰5æ¡
                    st.markdown(f"**è²¼æ–‡ {i+1}**")
                    st.write(f"ğŸ“… æ—¥æœŸ: {post.get('date', 'æœªçŸ¥')}")
                    st.write(f"ğŸ“ å†…å®¹: {post.get('content', 'ç„¡å†…å®¹')[:200]}...")
                    if 'url' in post and post['url']:  # ç¡®ä¿URLå­˜åœ¨ä¸”ä¸ä¸ºç©º
                        st.markdown(f"[æŸ¥çœ‹åŸæ–‡]({post['url']})")
                    st.divider()
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°æ­£é¢è²¼æ–‡")
        
        with tab2:
            st.markdown("### ğŸ˜ æœ€å…·ä»£è¡¨æ€§çš„ä¸­æ€§è²¼æ–‡")
            neutral_posts = [post for post in social_data if post.get('sentiment') == 'ä¸­æ€§']
            if neutral_posts:
                # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                neutral_posts.sort(key=lambda x: x.get('date', ''), reverse=True)
                for i, post in enumerate(neutral_posts[:5]):  # æ˜¾ç¤ºå‰5æ¡
                    st.markdown(f"**è²¼æ–‡ {i+1}**")
                    st.write(f"ğŸ“… æ—¥æœŸ: {post.get('date', 'æœªçŸ¥')}")
                    st.write(f"ğŸ“ å†…å®¹: {post.get('content', 'ç„¡å†…å®¹')[:200]}...")
                    if 'url' in post and post['url']:
                        st.markdown(f"[æŸ¥çœ‹åŸæ–‡]({post['url']})")
                    st.divider()
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°ä¸­æ€§è²¼æ–‡")
        
        with tab3:
            st.markdown("### ğŸ˜” æœ€å…·ä»£è¡¨æ€§çš„è² é¢è²¼æ–‡")
            negative_posts = [post for post in social_data if post.get('sentiment') == 'è² é¢']
            if negative_posts:
                # æŒ‰æ—¥æœŸæ’åºï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
                negative_posts.sort(key=lambda x: x.get('date', ''), reverse=True)
                for i, post in enumerate(negative_posts[:5]):  # æ˜¾ç¤ºå‰5æ¡
                    st.markdown(f"**è²¼æ–‡ {i+1}**")
                    st.write(f"ğŸ“… æ—¥æœŸ: {post.get('date', 'æœªçŸ¥')}")
                    st.write(f"ğŸ“ å†…å®¹: {post.get('content', 'ç„¡å†…å®¹')[:200]}...")
                    if 'url' in post and post['url']:
                        st.markdown(f"[æŸ¥çœ‹åŸæ–‡]({post['url']})")
                    
                    # è´Ÿé¢å†…å®¹æä¾›å¸®åŠ©èµ„æº
                    content = str(post.get('content', ''))
                    if any(keyword in content for keyword in sensitive_keywords):
                        st.warning("""
                        ğŸš¨ é‡è¦æé†’ï¼š
                        é¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222
                        ç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000
                        æˆ‘å€‘éå¸¸é—œå¿ƒæ‚¨çš„å®‰å…¨ï¼
                        """)
                    
                    st.divider()
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°è² é¢è²¼æ–‡")
                
    except Exception as e:
        st.error(f"ç”¢ç”Ÿå ±å‘Šå¤±æ•—: {str(e)}")
        

# æ–°å¢å¿ƒçµå»ºè®®ç”Ÿæˆå‡½æ•°
def generate_advice(report_summary):
    """æ ¹æ®æŠ¥å‘Šæ‘˜è¦ç”Ÿæˆå»ºè®¾æ€§å»ºè®®"""
    # åˆ›å»ºæ›´æ˜ç¡®çš„æç¤ºè¯
    prompt = ("ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¿ƒç†å¥åº·é¡¾é—®ï¼Œè¯·æ ¹æ®ä»¥ä¸‹æƒ…æ„Ÿåˆ†ææŠ¥å‘Šï¼Œä¸ºç”¨æˆ·æä¾›ä¸€å¿ƒç†å¥åº·å»ºè®®ï¼š{report_summary}ã€‚è¯·è€ƒè™‘ä»¥ä¸‹å› ç´ ï¼š1. æ­£é¢/ä¸­æ€§/è´Ÿé¢æƒ…ç»ªçš„æ¯”ä¾‹åˆ†å¸ƒ 2. æ—¶é—´è¶‹åŠ¿ä¸­çš„å…³é”®å‘ç°ï¼ˆå¦‚é«˜å³°æ—¥ã€è¿‘æœŸå˜åŒ–è¶‹åŠ¿ï¼‰ 3. ç‰¹æ®Šéœ€æ±‚å†…å®¹çš„æ•°é‡å’Œç±»å‹ã€‚ä½¿ç”¨æ¸©æš–ã€æ”¯æŒæ€§çš„è¯­æ°”ï¼Œç”¨ç¹ä½“ä¸­æ–‡å›å¤ã€‚å»ºè®®åº”åŒ…å«ï¼š- æƒ…ç»ªç®¡ç†æŠ€å·§ - æ—¥å¸¸è‡ªæˆ‘ç…§é¡¾æ–¹æ³• - ç¤¾äº¤æ”¯æŒå»ºè®® - ä¸“ä¸šæ±‚åŠ©æŒ‡å¼•ï¼ˆå¦‚éœ€è¦ï¼‰"
    )
    
    try:
        # è°ƒç”¨æ¨¡å‹ç”Ÿæˆå»ºè®®
        response, _ = model.chat(
            tokenizer, 
            query=prompt, 
            history=None, 
            max_length=2048,
            temperature=0.7,
            top_p=0.9
        )
        
        # ç¡®ä¿è¿”å›å®Œæ•´çš„å»ºè®®å†…å®¹
        advice = convert_to_traditional_chinese(response)
        return advice
    except Exception as e:
        error_msg = f"å»ºè­°ç”Ÿæˆå¤±æ•—: {str(e)}"
        return f"## âŒ éŒ¯èª¤\n\n{error_msg}"

# ç”Ÿæˆå¹¶æ˜¾ç¤ºå¿ƒçµå»ºè®®
    st.subheader("ğŸ’– å°ˆæ¥­å¿ƒç†å¥åº·å»ºè­°")
    with st.spinner("æ­£åœ¨ç”Ÿæˆå€‹æ€§åŒ–å»ºè­°..."):
        mental_health_advice = generate_advice(report_summary)
        st.markdown(mental_health_advice)
    
        # æ·»åŠ ç´§æ€¥æ±‚åŠ©ä¿¡æ¯
        st.warning("""
        **ğŸš¨ ç·Šæ€¥æ”¯æ´ï¼š**
        å¦‚æœæ‚¨æˆ–æ‚¨èªè­˜çš„äººæ­£åœ¨ç¶“æ­·å›°é›£æ™‚æœŸï¼Œè«‹ç«‹å³å°‹æ±‚å¹«åŠ©ï¼š
        - é¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222
        - ç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000
        - é†«é™¢ç®¡ç†å±€ç²¾ç¥å¥åº·å°ˆç·šï¼š+852 2466 7350
        """)
        
        # æ·»åŠ æœ¬åœ°èµ„æºé“¾æ¥
        st.info("""
        **ğŸ¥ æœ¬åœ°å¿ƒç†å¥åº·è³‡æºï¼š**
        - é¦™æ¸¯å¿ƒç†è¡ç”Ÿæœƒï¼šhttps://www.mhahk.org.hk/
        - é¦™æ¸¯å¿ƒè†ï¼šhttps://www.jciconcern.hk/
        - æ˜æ„›å®¶åº­æœå‹™ï¼šhttps://family.caritas.org.hk/
        - é¦™æ¸¯ç²¾ç¥ç§‘é†«å­¸é™¢ï¼šhttps://www.hkcp.org/
        """)

def generate_local_resources(negative_percentage):
    """æ ¹æ®è´Ÿé¢æƒ…ç»ªæ¯”ä¾‹æ¨èæœ¬åœ°èµ„æº"""
    # åŸºç¡€èµ„æº
    resources = {
        "ç·Šæ€¥ç†±ç·š": [
            "é¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222",
            "ç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000"
        ],
        "å¿ƒç†è«®è©¢æœå‹™": [
            "é¦™æ¸¯å¿ƒç†è¡›ç”Ÿæœƒï¼šhttps://www.mhahk.org.hk/",
            "æ˜æ„›å¿ƒç†å¥åº·æœå‹™ï¼šhttps://mentalhealth.caritas.org.hk/"
        ]
    }
    
    # æ ¹æ®è´Ÿé¢æƒ…ç»ªæ¯”ä¾‹æ·»åŠ é¢å¤–èµ„æº
    if negative_percentage > 20:
        resources["ç¤¾å€æ”¯æ´ä¸­å¿ƒ"] = [
            "æ±è¯ä¸‰é™¢å¿ƒç†å¥åº·æœå‹™ï¼šhttps://www.tungwah.org.hk/",
            "é¦™æ¸¯é’å¹´å”æœƒè¼”å°ä¸­å¿ƒï¼šhttps://www.hkfyg.org.hk/"
        ]
    
    if negative_percentage > 30:
        resources["å°ˆæ¥­å¿ƒç†æ²»ç™‚"] = [
            "é¦™æ¸¯å¿ƒç†å­¸æœƒèªå¯å¿ƒç†å­¸å®¶åå–®ï¼šhttps://www.dcp.hkps.org.hk/",
            "é†«é™¢ç®¡ç†å±€ç²¾ç¥å¥åº·æœå‹™ï¼šhttps://www.ha.org.hk/"
        ]
    
    # æ·»åŠ è‡ªæˆ‘æå‡èµ„æº
    resources["è‡ªæˆ‘æå‡è³‡æº"] = [
        "é¦™æ¸¯å…¬å…±åœ–æ›¸é¤¨å¿ƒç†å¥åº·æ›¸ç±å°ˆå€",
        "Mindfulness HK æ­£å¿µèª²ç¨‹ï¼šhttps://mindfulnesshongkong.com/",
        "OpenUpå¿ƒç†æ”¯æ´å¹³å°ï¼šhttps://openup.hk/"
    ]
    
    return resources

def generate_comprehensive_report():
    """ç”¢ç”Ÿæ•´åˆç¤¾ç¾¤åª’é«”æ•¸æ“šå’Œä½¿ç”¨è€…å°è©±çš„ç¶œåˆæƒ…æ„Ÿåˆ†æå ±å‘Š"""
    try:
        all_data = []
        
        # æ·»åŠ ç¤¾äº¤åª’ä½“æ•°æ®
        if 'social_data' in st.session_state:
            all_data.extend(st.session_state.social_data)
        
        # æ·»åŠ ç”¨æˆ·å¯¹è¯æ•°æ®
        if 'user_inputs' in st.session_state and st.session_state.user_inputs:
            for input_data in st.session_state.user_inputs:
                # æ–°å¢: æ·»åŠ æ ‡ç­¾å­—æ®µ
                tags = []
                content = str(input_data.get('content', ''))
                
                # æ£€æŸ¥æ•æ„Ÿè¯
                if any(keyword in content for keyword in sensitive_keywords):
                    tags.append('æ•æ„Ÿå†…å®¹')
                
                # æ£€æŸ¥äº¤å‹éœ€æ±‚
                if any(phrase in content for phrase in friendship_keywords):
                    tags.append('äº¤å‹éœ€æ±‚')
                
                all_data.append({
                    'platform': 'ä½¿ç”¨è€…å°è©±',
                    'content': content,
                    'date': input_data['date'],
                    'tags': tags  # æ–°å¢æ ‡ç­¾å­—æ®µ
                })
        
        if not all_data:
            st.warning("æ²’æœ‰å¯åˆ†æçš„æ•¸æ“šï¼Œè«‹å…ˆåˆ†æç¤¾ç¾¤åª’é«”æˆ–é€²è¡Œå°è©±")
            return
        
        # æ‰“å°è°ƒè¯•ä¿¡æ¯
        st.write(f"ç¸½æ•¸æ“šé‡: {len(all_data)}æ¢")
        st.json(all_data[:2])  # æ˜¾ç¤ºå‰2æ¡æ•°æ®ç”¨äºè°ƒè¯•
        
        # æå–æ–‡æœ¬å†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†æ
        contents = [item.get('content', '') for item in all_data]
        
        # æ·»åŠ åŠ è½½çŠ¶æ€æŒ‡ç¤ºå™¨
        with st.spinner("æ­£åœ¨åˆ†ææƒ…æ„Ÿï¼Œè«‹ç¨å€™..."):
            try:
                # æ·»åŠ APIè°ƒç”¨é™åˆ¶å¤„ç†
                sentiment_result = HanLP.sentiment_analysis(contents)
                st.session_state.sentiment_result = sentiment_result
            except Exception as e:
                if "rate limit" in str(e).lower():
                    st.error("æƒ…ç·’åˆ†æAPIå‘¼å«éæ–¼é »ç¹ï¼Œè«‹ç¨å¾Œå†è©¦")
                else:
                    st.error(f"æƒ…ç·’åˆ†æå¤±æ•—: {str(e)}")
                return
        
        # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾åˆ°æ¯æ¡æ•°æ® - ä½¿ç”¨ä¸Instagram/Twitterç›¸åŒçš„é˜ˆå€¼
        for i, item in enumerate(all_data):
            if i < len(sentiment_result):
                # å¤„ç†æƒ…æ„Ÿåˆ†æç»“æœå¯èƒ½æ˜¯æ•°å€¼çš„æƒ…å†µ
                sentiment_value = sentiment_result[i]
                if isinstance(sentiment_value, float):
                    # ä½¿ç”¨ä¸Instagram/Twitterç›¸åŒçš„é˜ˆå€¼æ ‡å‡†
                    if sentiment_value > 0.6:
                        item['sentiment'] = 'æ­£é¢'
                    elif sentiment_value < 0.4:
                        item['sentiment'] = 'è² é¢'
                    else:
                        item['sentiment'] = 'ä¸­æ€§'
                else:
                    # å¦‚æœè¿”å›çš„æ˜¯å­—ç¬¦ä¸²æ ‡ç­¾ï¼Œç›´æ¥ä½¿ç”¨
                    item['sentiment'] = sentiment_value
            else:
                item['sentiment'] = 'æœªçŸ¥'
                
        sentiment_labels = [item.get('sentiment', 'æœªçŸ¥') for item in all_data]
        sentiment_counts = pd.Series(sentiment_labels).value_counts()
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        total = len(all_data)
        positive_percentage = round(sentiment_counts.get('æ­£é¢', 0) / total * 100, 1) if total > 0 else 0
        neutral_percentage = round(sentiment_counts.get('ä¸­æ€§', 0) / total * 100, 1) if total > 0 else 0
        negative_percentage = round(sentiment_counts.get('è² é¢', 0) / total * 100, 1) if total > 0 else 0
        
        # åˆ†ææƒ…æ„Ÿè¶‹åŠ¿
        trend_analysis = "ç©©å®š"
        if negative_percentage > 30:
            trend_analysis = "éœ€è¦æ³¨æ„è² é¢æƒ…ç·’è¼ƒå¤š"
        elif negative_percentage > 50:
            trend_analysis = "è² é¢æƒ…ç·’ä½”ä¸»å°ï¼Œå»ºè­°å°‹æ±‚å°ˆæ¥­å”åŠ©"
        
        # åˆ›å»ºæƒ…æ„Ÿåˆ†å¸ƒé¥¼å›¾
        st.subheader("æƒ…æ„Ÿåˆ†å¸ƒåˆ†æ")
        # æå–æƒ…æ„Ÿæ ‡ç­¾
        sentiment_labels = [item['sentiment'] for item in all_data]
        sentiment_counts = pd.Series(sentiment_labels).value_counts()
        
        if not sentiment_counts.empty:
            fig1 = px.pie(
                sentiment_counts, 
                values=sentiment_counts.values,
                names=sentiment_counts.index,
                title="æƒ…æ„Ÿåˆ†å¸ƒæ¯”ä¾‹",
                color_discrete_sequence=px.colors.qualitative.Pastel
            )
            fig1.update_traces(textposition='inside', textinfo='percent+label')
            st.plotly_chart(fig1, use_container_width=True)
        else:
            st.warning("æ²’æœ‰è¶³å¤ çš„æƒ…ç·’æ•¸æ“šé€²è¡Œåˆ†æ")
        
        # æŒ‰å¹³å°åˆ†ææƒ…æ„Ÿåˆ†å¸ƒ - ä¿®å¤ç©ºæ•°æ®é—®é¢˜
        st.subheader("å„å¹³å°æƒ…æ„Ÿåˆ†å¸ƒ")
        if all_data:
            df = pd.DataFrame(all_data)
            
            # ç¡®ä¿å¹³å°åˆ—å­˜åœ¨
            if 'platform' in df.columns and 'sentiment' in df.columns:
                # åˆ›å»ºå¹³å°-æƒ…æ„Ÿçš„äº¤å‰è¡¨
                platform_sentiment = pd.crosstab(
                    df['platform'], 
                    df['sentiment']
                )
                
                # ç¡®ä¿æ‰€æœ‰å¹³å°éƒ½æœ‰ç›¸åŒçš„åˆ—æ•°
                if not platform_sentiment.empty:
                    # è·å–æ‰€æœ‰å¯èƒ½çš„æƒ…æ„Ÿå€¼
                    all_sentiments = ['æ­£é¢', 'ä¸­æ€§', 'è² é¢']
                    
                    # æ·»åŠ ç¼ºå¤±çš„æƒ…æ„Ÿåˆ—
                    for sentiment in all_sentiments:
                        if sentiment not in platform_sentiment.columns:
                            platform_sentiment[sentiment] = 0
                    
                    # é‡æ–°æ’åºåˆ—
                    platform_sentiment = platform_sentiment[all_sentiments]
                    
                    # é‡ç½®ç´¢å¼•
                    platform_sentiment = platform_sentiment.reset_index()
                    
                    # è½¬æ¢æ•°æ®ä¸ºé•¿æ ¼å¼
                    melted_data = platform_sentiment.melt(
                        id_vars=['platform'], 
                        value_vars=all_sentiments,
                        var_name='æƒ…æ„Ÿ',
                        value_name='æ•¸é‡'
                    )
                    
                    # åˆ›å»ºæ¡å½¢å›¾
                    fig2 = px.bar(
                        melted_data,
                        x='platform',
                        y='æ•¸é‡',
                        color='æƒ…æ„Ÿ',
                        title="å„å¹³å°æƒ…æ„Ÿåˆ†å¸ƒ",
                        labels={'platform': 'å¹³å°', 'æ•¸é‡': 'è²¼æ–‡æ•°é‡'},
                        barmode='group'
                    )
                    st.plotly_chart(fig2, use_container_width=True)
                else:
                    st.warning("æ²’æœ‰è¶³å¤ çš„å¹³å°æ•¸æ“šé€²è¡Œåˆ†æ")
            else:
                st.warning("æ•¸æ“šä¸­ç¼ºä¹å¿…è¦çš„å¹³å°æˆ–æƒ…æ„Ÿè¨Šæ¯")
        else:
            st.warning("æ²’æœ‰å¯ç”¨æ–¼å¹³å°åˆ†æçš„æ•¸æ“š")
        
        # æŒ‰æ—¶é—´åˆ†ææƒ…æ„Ÿè¶‹åŠ¿ - ä¿®å¤æ—¥æœŸå¤„ç†é”™è¯¯
        st.subheader("æƒ…ç·’è¶¨å‹¢åˆ†æ")
        if all_data:
            df = pd.DataFrame(all_data)
            
            # ç¡®ä¿æœ‰æ—¥æœŸå­—æ®µ
            if 'date' in df.columns and not df['date'].empty:
                try:
                    # ç»Ÿä¸€æ—¥æœŸæ ¼å¼å¤„ç†
                    def format_date(date_str):
                        try:
                            if isinstance(date_str, datetime):
                                return date_str
                            # å°è¯•å¤šç§æ—¥æœŸæ ¼å¼
                            for fmt in ("%Y-%m-%d %H:%M:%S", "%Y-%m-%d", "%d/%m/%Y", "%m/%d/%Y", "%Y/%m/%d"):
                                try:
                                    return datetime.strptime(str(date_str), fmt)
                                except ValueError:
                                    continue
                            return None
                        except:
                            return None
                    
                    # åº”ç”¨æ—¥æœŸæ ¼å¼åŒ–
                    df['formatted_date'] = df['date'].apply(format_date)
                    
                    # ç§»é™¤æ— æ•ˆæ—¥æœŸ
                    df = df.dropna(subset=['formatted_date'])
                    
                    if not df.empty:
                        # è½¬æ¢æ—¥æœŸæ ¼å¼
                        df['formatted_date'] = pd.to_datetime(df['formatted_date'])
                        df['date_only'] = df['formatted_date'].dt.date
                        
                        # æŒ‰æ—¥æœŸåˆ†ç»„ç»Ÿè®¡
                        daily_sentiment = df.groupby(['date_only', 'sentiment']).size().unstack(fill_value=0)
                        
                        # ç¡®ä¿æ‰€æœ‰æƒ…æ„Ÿç±»åˆ«éƒ½å­˜åœ¨
                        all_sentiments = ['æ­£é¢', 'ä¸­æ€§', 'è² é¢']
                        for sentiment in all_sentiments:
                            if sentiment not in daily_sentiment.columns:
                                daily_sentiment[sentiment] = 0
                        
                        # åˆ›å»ºæ¯æ—¥æƒ…æ„Ÿè¶‹åŠ¿å›¾
                        fig3 = px.line(
                            daily_sentiment.reset_index(),  # é‡ç½®ç´¢å¼•ç¡®ä¿æ•°æ®ç»“æ„ä¸€è‡´
                            x='date_only',
                            y=all_sentiments,
                            title="æƒ…ç·’è®ŠåŒ–è¶¨å‹¢",
                            labels={'value': 'æ•¸é‡', 'date_only': 'æ—¥æœŸ'},
                            markers=True
                        )
                        fig3.update_layout(
                            legend_title_text='æƒ…æ„Ÿé¡å‹',
                            xaxis_title='æ—¥æœŸ',
                            yaxis_title='æ•¸é‡'
                        )
                        st.plotly_chart(fig3, use_container_width=True)
                    else:
                        st.warning("æ²’æœ‰æœ‰æ•ˆçš„æ—¥æœŸæ•¸æ“š")
                except Exception as e:
                    st.error(f"æ—¥æœŸè™•ç†éŒ¯èª¤: {str(e)}")
            else:
                st.warning("æ•¸æ“šä¸­ç¼ºå°‘æ—¥æœŸä¿¡æ¯")
        else:
            st.warning("æ²’æœ‰å¯ç”¨æ–¼è¶¨å‹¢åˆ†æçš„æ•¸æ“š")
        
        # æƒ…æ„Ÿå…³é”®è¯åˆ†æ - ä¿®å¤ç©ºæ•°æ®é—®é¢˜
        st.subheader("æƒ…ç·’é—œéµå­—åˆ†æ")
        col1, col2, col3 = st.columns(3)
        
        # è¾…åŠ©å‡½æ•°ï¼šæå–å…³é”®è¯
        def extract_keywords(items, sentiment_name):
            if not items:
                return None
                
            try:
                # åˆå¹¶æ‰€æœ‰å†…å®¹
                combined_text = " ".join([str(item.get('content', '')) for item in items])
                if not combined_text.strip():
                    return None
                
                # æå–å…³é”®è¯
                keywords = HanLP.keyphrase_extraction(combined_text)
                return keywords
            except Exception as e:
                st.error(f"{sentiment_name}é—œéµå­—æ“·å–å¤±æ•—: {str(e)}")
                return None
        
        # æ˜¾ç¤ºä»£è¡¨æ€§å†…å®¹ - ä¿®å¤ç©ºæ•°æ®é—®é¢˜
        st.subheader("ä»£è¡¨æ€§å†…å®¹åˆ†æ")
        tab1, tab2, tab3, tab4 = st.tabs(["æ­£é¢å†…å®¹", "ä¸­æ€§å†…å®¹", "è² é¢å†…å®¹", "ç‰¹æ®Šéœ€æ±‚"])
        
        # è¾…åŠ©å‡½æ•°ï¼šæ˜¾ç¤ºä»£è¡¨æ€§å†…å®¹
        def display_representative_items(items, sentiment_name):
            """é¡¯ç¤ºä»£è¡¨æ€§å…§å®¹çš„è¼”åŠ©å‡½æ•¸"""
            if not items:
                st.info(f"æ²¡æœ‰æ‰¾åˆ°{sentiment_name}å†…å®¹")
                return
                    
            # æŒ‰å†…å®¹é•¿åº¦æ’åºï¼ˆå‡è®¾é•¿å†…å®¹æ›´æœ‰ä»£è¡¨æ€§ï¼‰
            items.sort(key=lambda x: len(str(x.get('content', ''))), reverse=True)
            for i, item in enumerate(items[:5]):  # æ˜¾ç¤ºå‰5æ¡
                st.markdown(f"**æ¥æº: {item.get('platform', 'æœªçŸ¥')}**")
                if item.get('date'):
                    st.write(f"ğŸ“… æ—¥æœŸ: {item.get('date')}")
                
                content = str(item.get('content', 'ç„¡å†…å®¹'))
                st.write(f"ğŸ“ å†…å®¹: {content[:300]}{'...' if len(content) > 300 else ''}")
                
                # æ˜¾ç¤ºæ ‡ç­¾
                tags = item.get('tags', [])
                if tags:
                    st.write(f"ğŸ·ï¸ æ¨™ç±¤: {', '.join(tags)}")
                
                if 'url' in item and item['url']:  # ç¡®ä¿URLå­˜åœ¨ä¸”ä¸ä¸ºç©º
                    st.markdown(f"[æŸ¥çœ‹åŸæ–‡]({item['url']})")
                
                # æ•æ„Ÿå†…å®¹æä¾›å¸®åŠ©èµ„æº
                if any(keyword in content for keyword in sensitive_keywords):
                    st.warning("""
                    ğŸš¨ é‡è¦æé†’ï¼š
                    é¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222
                    ç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000
                    æˆ‘å€‘éå¸¸é—œå¿ƒæ‚¨çš„å®‰å…¨ï¼
                    """)
                
                st.divider()
        
        # ä½¿ç”¨è¾…åŠ©å‡½æ•°æ˜¾ç¤ºå„ç±»å†…å®¹
        with tab1:
            st.markdown("### ğŸ˜Š æœ€å…·ä»£è¡¨æ€§çš„æ­£é¢å†…å®¹")
            positive_items = [item for item in all_data if item.get('sentiment') == 'æ­£é¢']
            display_representative_items(positive_items, "æ­£é¢")
        
        with tab2:
            st.markdown("### ğŸ˜ æœ€å…·ä»£è¡¨æ€§çš„ä¸­æ€§å†…å®¹")
            neutral_items = [item for item in all_data if item.get('sentiment') == 'ä¸­æ€§']
            display_representative_items(neutral_items, "ä¸­æ€§")
        
        with tab3:
            st.markdown("### ğŸ˜” æœ€å…·ä»£è¡¨æ€§çš„è² é¢å†…å®¹")
            negative_items = [item for item in all_data if item.get('sentiment') == 'è² é¢']
            display_representative_items(negative_items, "è² é¢")
            
        # æ–°å¢: ç‰¹æ®Šéœ€æ±‚æ ‡ç­¾é¡µ
        with tab4:
            st.markdown("### ğŸ¤ ç‰¹æ®Šéœ€æ±‚å†…å®¹")
            special_items = []
            
            # ç­›é€‰æœ‰ç‰¹æ®Šæ ‡ç­¾çš„å†…å®¹
            for item in all_data:
                tags = item.get('tags', [])
                if 'äº¤å‹éœ€æ±‚' in tags or 'æ•æ„Ÿå†…å®¹' in tags:
                    special_items.append(item)
            
            if special_items:
                for i, item in enumerate(special_items):
                    st.markdown(f"**å†…å®¹ {i+1}**")
                    st.write(f"ğŸ“… æ—¥æœŸ: {item.get('date', 'æœªçŸ¥')}")
                    
                    content = str(item.get('content', 'ç„¡å†…å®¹'))
                    st.write(f"ğŸ“ å†…å®¹: {content[:300]}{'...' if len(content) > 300 else ''}")
                    
                    # æ˜¾ç¤ºæ ‡ç­¾
                    tags = item.get('tags', [])
                    if tags:
                        st.write(f"ğŸ·ï¸ æ¨™ç±¤: {', '.join(tags)}")
                    
                    # äº¤å‹éœ€æ±‚æ˜¾ç¤ºç¤¾åŒºèµ„æº
                    if 'äº¤å‹éœ€æ±‚' in tags:
                        st.info("""
                        ğŸ¤ ç¤¾å€ä¸­å¿ƒè³‡æºï¼š
                        - é¦™æ¸¯é’å¹´å”æœƒï¼šhttps://www.hkfyg.org.hk/
                        - æ˜æ„›ç¤¾å€ä¸­å¿ƒï¼šhttps://www.caritas.org.hk/
                        - é¦™æ¸¯éŠæ¨‚å ´å”æœƒï¼šhttps://www.hkpa.hk/
                        - æ±è¯ä¸‰é™¢ç¤¾å€ä¸­å¿ƒï¼šhttps://www.tungwah.org.hk/
                        """)
                    
                    # æ•æ„Ÿå†…å®¹æä¾›å¸®åŠ©èµ„æº
                    if any(keyword in content for keyword in sensitive_keywords):
                        st.warning("""
                        ğŸš¨ é‡è¦æé†’ï¼š
                        é¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222
                        ç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000
                        æˆ‘å€‘éå¸¸é—œå¿ƒæ‚¨çš„å®‰å…¨ï¼
                        """)
                    
                    st.divider()
            else:
                st.info("æ²¡æœ‰æ‰¾åˆ°ç‰¹æ®Šéœ€æ±‚å†…å®¹")
                
        st.subheader("âœ¨ å¿ƒç†å¥åº·å»ºè­°")
        
        # åˆ›å»ºæŠ¥å‘Šæ‘˜è¦
        report_summary = f"""
        ## æƒ…æ„Ÿåˆ†ææ‘˜è¦
        - æ­£é¢æƒ…ç·’: {positive_percentage}%
        - ä¸­æ€§æƒ…ç·’: {neutral_percentage}%
        - è² é¢æƒ…ç·’: {negative_percentage}%
        
        ## ä¸»è¦è¶¨å‹¢
        {trend_analysis}
        
        ## ä¸»è¦è§€å¯Ÿ
        {f"æª¢æ¸¬åˆ°{len(special_items)}æ¢ç‰¹æ®Šéœ€æ±‚å…§å®¹" if 'special_items' in locals() else "æœªæª¢æ¸¬åˆ°ç‰¹æ®Šéœ€æ±‚å…§å®¹"}
        """
        
        # ç”ŸæˆAIå»ºè®®
        with st.spinner("æ­£åœ¨ç”Ÿæˆå°ˆæ¥­å»ºè­°..."):
            advice = generate_advice(report_summary)
            formatted_advice = f"## ğŸ“ å°ˆæ¥­å»ºè­°\n\n{advice}"
            st.markdown(formatted_advice, unsafe_allow_html=True)
            st.text(advice)
        
        # æ·»åŠ æœ¬åœ°èµ„æºæ¨è
        st.subheader("ğŸ¥ æœ¬åœ°æ”¯æ´è³‡æº")
        resources = generate_local_resources(negative_percentage)
        
        # ä½¿ç”¨tabsæ›¿ä»£åµŒå¥—çš„expander
        resource_tabs = st.tabs(list(resources.keys()))
        
        for tab, (category, items) in zip(resource_tabs, resources.items()):
            with tab:
                for item in items:
                    st.write(f"- {item}")
        threading.Thread(target=text_to_speech, args=("å ±å‘Šå®Œæˆäº†ï¼Œå¿«é»å»çœ‹çœ‹å§", 'zh-TW', 'gentle_female', True)).start()
        
        # ä¿å­˜å®Œæ•´åˆ†ææŠ¥å‘Š
        if st.button("ğŸ’¾ ä¿å­˜å®Œæ•´å ±å‘Š", key="save_report_button"):
            # åˆ›å»ºä¿å­˜ç›®å½•
            if not os.path.exists("./reports"):
                os.makedirs("./reports")
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"./reports/full_report_{timestamp}.json"
            
            try:
                # ä¿å­˜æ•°æ®
                with open(filename, "w", encoding="utf-8") as f:
                    json.dump(all_data, f, indent=4, ensure_ascii=False)
                
                st.success(f"å®Œæ•´å ±å‘Šå·²ä¿å­˜åˆ°: {filename}")
            except Exception as e:
                st.error(f"ä¿å­˜å ±å‘Šå¤±æ•—: {str(e)}")
    
    except Exception as e:
        st.error(f"ç”¢ç”Ÿç¶œåˆå ±å‘Šæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        import traceback
        st.text(traceback.format_exc())  # æ‰“å°å®Œæ•´å †æ ˆè·Ÿè¸ª

def sister_style_transform(text):
    try:
        # ä½¿ç”¨å¤§æ¨¡å‹è‡ªèº«çš„èƒ½åŠ›è¿›è¡Œé£æ ¼è½¬æ¢
        prompt = (
            "å—¨ï¼Œæˆ‘æ˜¯ä½ çš„çŸ¥å¿ƒå§å§ã€‚è®©æˆ‘ç”¨æ¸©å’Œçš„è¯­æ°”æ¥é™ªä¼´ä½ ï¼š\n"
            f"{text}"
        )
        
        # è°ƒç”¨æ¨¡å‹è¿›è¡Œé£æ ¼è½¬æ¢
        styled_response, _ = model.chat(
            tokenizer, 
            query=prompt, 
            history=None, 
            max_length=2048
        )
        
        return styled_response
    except Exception as e:
        print(f"é¢¨æ ¼è½‰æ›å¤±æ•—: {str(e)}")
        return text  # å¤±è´¥æ—¶è¿”å›åŸå§‹æ–‡æœ¬

def convert_to_traditional_chinese(text):
    cc = OpenCC('s2t')  # å°†ç®€ä½“ä¸­æ–‡è½¬æ¢ä¸ºç¹ä½“ä¸­æ–‡
    traditional_text = cc.convert(text)
    return traditional_text

def play_audio(file_path):
    """æ’­æ”¾éŸ³è¨Šæª”æ¡ˆ"""
    try:
        pygame.mixer.init()
        pygame.mixer.music.load(file_path)
        pygame.mixer.music.play()
        while pygame.mixer.music.get_busy():
            pygame.time.Clock().tick(10)
    except Exception as e:
        print(f"éŸ³è¨Šæ’­æ”¾å¤±æ•—: {str(e)}")
    finally:
        try:
            os.remove(file_path)
        except:
            pass

def online_tts(text, lang='zh-TW', slow=False, pitch=1.0):
    """
    ä½¿ç”¨ Google TTS ç”¢ç”Ÿæº«æŸ”å¥³è²ï¼ˆéœ€è¦ç¶²è·¯é€£ç·šï¼‰
    
     åƒæ•¸:
     text: è¦è½‰æ›çš„æ–‡æœ¬
     lang: èªè¨€ä»£ç¢¼ (zh-TW: ç¹é«”ä¸­æ–‡, zh-CN: ç°¡é«”ä¸­æ–‡)
     slow: æ˜¯å¦æ”¾æ…¢èªé€Ÿ
     pitch: éŸ³èª¿èª¿æ•´ (1.0 æ­£å¸¸, >1.0 æ›´é«˜, <1.0 æ›´ä½)
     """
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".mp3", delete=False) as fp:
            temp_file = fp.name
        
        # ä½¿ç”¨ gTTS ç”Ÿæˆè¯­éŸ³
        tts = gTTS(text=text, lang=lang, slow=slow)
        tts.save(temp_file)
        return temp_file
    except Exception as e:
        print(f"ç·šä¸ŠTTSå¤±æ•—: {str(e)}")
        return None
        
        return temp_file
    except Exception as e:
        print(f"ç·šä¸ŠTTSå¤±æ•—: {str(e)}")
        return None
    
def offline_tts(text, lang='zh-tw', pitch=110, rate=150, volume=0.9):
    """
     ä½¿ç”¨ pyttsx3 é›¢ç·šç”¢ç”ŸèªéŸ³
    
     åƒæ•¸:
     text: è¦è½‰æ›çš„æ–‡æœ¬
     lang: èªè¨€ç¨‹å¼ç¢¼
     pitch: éŸ³ (50-200, é è¨­110)
     rate: èªé€Ÿ (100-300, é è¨­150)
     volume: éŸ³é‡ (0.0-1.0, é è¨­0.9)
     """
    try:
        # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as fp:
            temp_file = fp.name
        
        # åˆå§‹åŒ–å¼•æ“
        engine = pyttsx3.init()
        
        # å°è¯•è®¾ç½®å¥³æ€§å£°éŸ³
        voices = engine.getProperty('voices')
        female_voices = []
        
        for voice in voices:
            if 'female' in voice.name.lower() or 'woman' in voice.name.lower():
                if 'chinese' in voice.name.lower() or 'zh' in voice.id.lower():
                    engine.setProperty('voice', voice.id)
                    break
                else:
                    female_voices.append(voice)
        
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸­æ–‡å¥³æ€§å£°éŸ³ï¼Œä½¿ç”¨ç¬¬ä¸€ä¸ªå¥³æ€§å£°éŸ³
        if not engine.getProperty('voice') and female_voices:
            engine.setProperty('voice', female_voices[0].id)
        
        # è®¾ç½®è¯­éŸ³å‚æ•°
        engine.setProperty('rate', rate)
        engine.setProperty('volume', volume)
        engine.setProperty('pitch', pitch)  # è°ƒæ•´éŸ³è°ƒ
        
        # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        engine.save_to_file(text, temp_file)
        engine.runAndWait()
        
        return temp_file
    except Exception as e:
        print(f"é›¢ç·šTTSå¤±æ•—: {str(e)}")
        return None
    
def text_to_speech(text, lang='zh-TW', voice_type='gentle_female', online_first=True):
    """
     æ–‡å­—è½‰èªéŸ³ä¸»å‡½æ•¸
    
     åƒæ•¸:
     text: è¦è½‰æ›çš„æ–‡æœ¬
     lang: èªè¨€ç¨‹å¼ç¢¼
     voice_type: è²éŸ³é¡å‹ (gentle_female: æº«æŸ”å¥³è²)
     online_first: æ˜¯å¦å„ªå…ˆä½¿ç”¨ç·šä¸Šæœå‹™
     """
    if not text:
        return
    
    # æ¸©æŸ”å¥³å£°çš„å‚æ•°è®¾ç½®
    if voice_type == 'gentle_female':
        online_params = {'slow': 0, 'pitch': 1.1}  # ç¨æ…¢çš„è¯­é€Ÿå’Œç¨é«˜çš„éŸ³è°ƒ
        offline_params = {'pitch': 110, 'rate': 150, 'volume': 0.9}
    
    try:
        # ä¼˜å…ˆå°è¯•åœ¨çº¿æœåŠ¡
        if online_first and check_internet_connection():
            audio_file = online_tts(text, lang=lang, **online_params)
            if audio_file:
                play_audio(audio_file)
                return
    except:
        pass
    
    # åœ¨çº¿æœåŠ¡å¤±è´¥æˆ–ä¸å¯ç”¨æ—¶ä½¿ç”¨ç¦»çº¿æœåŠ¡
    audio_file = offline_tts(text, lang=lang, **offline_params)
    if audio_file:
        play_audio(audio_file)

def check_internet_connection(url="http://www.google.com", timeout=3):
    """æª¢æŸ¥ç¶²è·¯é€£ç·šæ˜¯å¦å¯ç”¨"""
    try:
        requests.get(url, timeout=timeout)
        return True
    except requests.ConnectionError:
        return False

def answer(user_history, bot_history, sample=True, top_p=0.75, temperature=0.95):
    '''sampleï¼šæ˜¯å¦æŠ½æ¨£ã€‚ç”Ÿæˆä»»å‹™ï¼Œå¯ä»¥è¨­å®šç‚ºTrue;
     top_pï¼š0-1ä¹‹é–“ï¼Œç”¢ç”Ÿçš„å…§å®¹è¶Šå¤šæ¨£åŒ–
    max_new_tokens=512 lost...'''

    if len(bot_history)>0:
        dialog_turn = 5 # è®¾ç½®å†å²å¯¹è¯è½®æ•°
        if len(bot_history)>dialog_turn:
            bot_history = bot_history[-dialog_turn:]
            user_history = user_history[-(dialog_turn+1):]
        
        context = "\n".join([f"ç”¨æˆ·ï¼š{user_history[i]}\nå¿ƒç†è«®è©¢å¸«ï¼š{bot_history[i]}" for i in range(len(bot_history))])
        input_text = context + "\nç”¨æˆ·ï¼š" + user_history[-1] + "\nå¿ƒç†è«®å•†å¸«ï¼š"
    else:
        input_text = "ç”¨æˆ·ï¼š" + user_history[-1] + "\nå¿ƒç†è«®å•†å¸«ï¼š"
        return "ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å€‹äººå°ˆå±¬æ•¸ä½è¼”å°è€å¸«ç”œå¿ƒè€å¸«ï¼Œæ­¡è¿æ‰¾æˆ‘å‚¾è¨´ã€è«‡å¿ƒï¼ŒæœŸå¾…å¹«åŠ©ä½ ï¼"
    
    print(input_text)
    if not sample:
        response, history = model.chat(tokenizer, query=input_text, history=None, max_length=2048, num_beams=1, do_sample=False, top_p=top_p, temperature=temperature, logits_processor=None)
    else:
        response, history = model.chat(tokenizer, query=input_text, history=None, max_length=2048, num_beams=1, do_sample=True, top_p=top_p, temperature=temperature, logits_processor=None)

    Traditionalresponse = convert_to_traditional_chinese(response)
    print("æ¨¡å‹åŸå§‹è¾“å‡ºï¼š\n", Traditionalresponse)
    
    Traditionalresponse_ex = sister_style_transform(Traditionalresponse)
    Finalresponse = re.sub("\n+", "\n", Traditionalresponse_ex)
    print('å¿ƒç†è«®å•†å¸«: '+Finalresponse)
    FinalTraditionalresponse = convert_to_traditional_chinese(Finalresponse)
    return Traditionalresponse + FinalTraditionalresponse
    
if 'show_comprehensive_report' in st.session_state and st.session_state.show_comprehensive_report:
    st.session_state.report_showing = True 
    st.session_state.report_displayed = True  # æ–°å¢ï¼šè®¾ç½®æŠ¥å‘Šæ˜¾ç¤ºçŠ¶æ€
    
    # ä½¿ç”¨å®¹å™¨ç¡®ä¿æŠ¥å‘Šæ˜¾ç¤ºå®Œæ•´
    with st.container():
        with st.expander("ğŸ“Š ç¶œåˆæƒ…æ„Ÿåˆ†æå ±å‘Š", expanded=True):
            generate_comprehensive_report()
            st.session_state.show_comprehensive_report = False
else:
    # å½“æŠ¥å‘Šä¸å†æ˜¾ç¤ºæ—¶ï¼Œé‡ç½®æŠ¥å‘Šæ˜¾ç¤ºçŠ¶æ€
    st.session_state.report_showing = False
    st.session_state.report_displayed = False  # æ–°å¢ï¼šé‡ç½®æŠ¥å‘Šæ˜¾ç¤ºçŠ¶æ€
    
    # é‡ç½®çŠ¶æ€ï¼Œé¿å…é‡å¤æ˜¾ç¤º
    st.session_state.show_comprehensive_report = False

# åˆå§‹åŒ– session_state ä¸­çš„ 'id'
if 'id' not in st.session_state:
    # åˆ›å»ºä¿å­˜ç”¨æˆ·èŠå¤©è®°å½•çš„ç›®å½•
    if not os.path.exists("./history"):
        os.makedirs("./history")
    json_files = os.listdir("./history")
    id = len(json_files)
    st.session_state['id'] = id
    
    
with st.sidebar:
    InstagramFind = "æ­£åœ¨ç²å–Instagramå…§å®¹ï¼Œè«‹ä½ ç­‰ä¸€ä¸‹å”·"
    TwitterFind = "æ­£åœ¨ç²å–Twitterå…§å®¹ï¼Œè«‹ä½ ç­‰ä¸€ä¸‹å”·"
    st.header("ğŸŒ ç¤¾ç¾¤åª’é«”æ•¸æ“šåˆ†æ")
    
    # å°†å¹³å°é€‰æ‹©ç§»å‡ºè¡¨å•
    platform = st.selectbox(
        "é¸æ“‡ç¤¾äº¤å¹³å°", 
        ["Instagram", "Twitter"],
        key="platform_select",
        on_change=lambda: st.session_state.update(platform_changed=True)
    )
    
    # æ£€æŸ¥å¹³å°æ˜¯å¦å·²æ›´æ”¹
    if 'platform_changed' in st.session_state and st.session_state.platform_changed:
        st.session_state.platform_changed = False
        st.session_state.form_cleared = True
        st.session_state.skip_audio = True
    
    # ä½¿ç”¨è¡¨å•å®¹å™¨
    with st.form("social_media_form"):
        # æ˜¾ç¤ºå½“å‰é€‰æ‹©çš„å¹³å°
        st.write(f"ç•¶å‰é¸æ“‡å¹³å°: **{platform}**")
        
        account_id = st.text_input("è¼¸å…¥å¸³è™ŸID", help="ä¾‹å¦‚ï¼šInstagramå¸³è™Ÿæˆ–Twitterç”¨æˆ·å")
        
        # å¹³å°ç‰¹å®šè®¾ç½® - åŠ¨æ€æ˜¾ç¤º
        if platform == "Instagram":
            download_num = st.number_input("ç²å–æœ€è¿‘å¹¾æ¢è²¼æ–‡", min_value=1, max_value=50, value=10)
        else:
            start_date = st.date_input("é–‹å§‹æ—¥æœŸ", value=datetime(2024, 1, 1))
            end_date = st.date_input("çµæŸæ—¥æœŸ", value=datetime.today())
        
        # åˆ†ææŒ‰é’®
        analyze_submitted = st.form_submit_button("ğŸ” åˆ†æå¸³è™Ÿå…§å®¹", 
                                                 help="é»æ“Šé–‹å§‹æŠ“å–å’Œåˆ†æç¤¾ç¾¤åª’é«”å…§å®¹")
        
        # ç”ŸæˆæŠ¥å‘ŠæŒ‰é’®
        report_submitted = st.form_submit_button("ğŸ“Š ç”Ÿæˆç»¼åˆå ±å‘Š", 
                                                help="é»æ“Šç”¢ç”Ÿæ•´åˆç¤¾ç¾¤åª’é«”å’Œä½¿ç”¨è€…å°è©±çš„ç¶œåˆå ±å‘Š")
        
    if 'social_data' not in st.session_state:
        st.session_state.social_data = []
    # å¤„ç†åˆ†ææŒ‰é’®ç‚¹å‡»
    if analyze_submitted:
        if not account_id:
            st.sidebar.warning("è«‹è¼¸å…¥å¸³è™ŸID")
        else:
            if platform == "Instagram":
                with st.spinner(InstagramFind):
                    threading.Thread(target=text_to_speech, args=(InstagramFind, 'zh-TW', 'gentle_female', True)).start()
                    try:
                        fetcher = InstagramDataFetcher(account_id, download_num)
                        social_data = fetcher.fetch_data()
                        
                        # æå–æ–‡æœ¬å†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†æ
                        captions = [item['content'] for item in social_data]
                        if captions:
                            sentiment_result = HanLP.sentiment_analysis(captions)
                            st.session_state.sentiment_result = sentiment_result
                        
                        # ç»Ÿä¸€å­˜å‚¨åˆ°session_state
                        st.session_state.social_data.extend(social_data)
                        
                        # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾åˆ°æ¯æ¡æ•°æ®
                        for i, item in enumerate(social_data):
                            if i < len(sentiment_result):
                                item['sentiment'] = sentiment_result[i]
                            else:
                                item['sentiment'] = 'æœªçŸ¥'
                        instagramdone = (f"æˆåŠŸæ‹¿åˆ°{len(social_data)}æ¢Instagramå…§å®¹ï¼å·²åŠ å…¥åˆ°ç¾æœ‰è³‡æ–™å•¦ã€‚")
                        st.success(instagramdone)
                        threading.Thread(target=text_to_speech, args=(instagramdone, 'zh-TW', 'gentle_female', True)).start()
                    except Exception as e:
                        st.error(f"Instagramæ•¸æ“šç²å–å¤±æ•—: {str(e)}")
            
            # Twitteréƒ¨åˆ†
            elif platform == "Twitter":
                with st.spinner(TwitterFind):
                    threading.Thread(target=text_to_speech, args=(TwitterFind, 'zh-TW', 'gentle_female', True)).start()
                    try:
                        scraper = TwitterExtractor(TWITTER_AUTH_TOKEN)
                        social_data = scraper.fetch_tweets(
                            f"https://twitter.com/{account_id}/likes",
                            start_date=start_date.strftime("%Y-%m-%d"),
                            end_date=end_date.strftime("%Y-%m-%d")
                        )
                        
                        for item in social_data:
                            item['platform'] = 'Twitter'
                            
                            # ä½¿ç”¨ç¿»è¯‘æ–‡æœ¬ä½œä¸ºä¸»è¦å†…å®¹
                            # å¦‚æœç¿»è¯‘æ–‡æœ¬å­˜åœ¨ä¸”ä¸æ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œåˆ™ä½¿ç”¨ç¿»è¯‘æ–‡æœ¬
                            if 'translated_text' in item and item['translated_text']:
                                item['content'] = item['translated_text']
                            else:
                                # å¦åˆ™ä½¿ç”¨åŸå§‹æ–‡æœ¬
                                item['content'] = item.get('text', '')
                        
                        # æå–æ–‡æœ¬å†…å®¹è¿›è¡Œæƒ…æ„Ÿåˆ†æ - ä½¿ç”¨ content å­—æ®µ
                        contents = [item.get('content', '') for item in social_data]
                        if contents:
                            sentiment_result = HanLP.sentiment_analysis(contents)
                            st.session_state.sentiment_result = sentiment_result
                        
                        st.session_state.social_data.extend(social_data)
                        
                        # æ·»åŠ æƒ…æ„Ÿæ ‡ç­¾åˆ°æ¯æ¡æ•°æ®
                        for i, item in enumerate(social_data):
                            if i < len(sentiment_result):
                                item['sentiment'] = sentiment_result[i]
                            else:
                                item['sentiment'] = 'æœªçŸ¥'
                        
                        Twitterdone = (f"æˆåŠŸæ‹¿åˆ°{len(social_data)}æ¢Twitterå…§å®¹ï¼å·²åŠ å…¥åˆ°ç¾æœ‰è³‡æ–™å•¦ã€‚")
                        st.success(Twitterdone)
                        threading.Thread(target=text_to_speech, args=(Twitterdone, 'zh-TW', 'gentle_female', True)).start()
                    except Exception as e:
                        st.error(f"Twitteræ•¸æ“šç²å–å¤±æ•—: {str(e)}")
            
    # å¤„ç†æŠ¥å‘ŠæŒ‰é’®ç‚¹å‡» - ç‹¬ç«‹çš„äº‹ä»¶å¤„ç†
    generatereporttext = "å ±å‘Šç”Ÿæˆä¸­ï¼Œç­‰æˆ‘ä¸€ä¸‹å§ï¼Œç­‰ç­‰å°±å¯ä»¥çœ‹å ±å‘Šäº†å•¦"
    if report_submitted:
        # ç¡®ä¿æœ‰æ•°æ®å¯åˆ†æ
        if 'social_data' in st.session_state or 'user_inputs' in st.session_state:
            # è®¾ç½®æŠ¥å‘Šæ˜¾ç¤ºçŠ¶æ€
            st.session_state.show_comprehensive_report = True
            
            # æ˜¾ç¤ºæç¤ºä¿¡æ¯
            st.sidebar.success(generatereporttext)
            
            # æ’­æ”¾è¯­éŸ³æç¤º
            threading.Thread(target=text_to_speech, args=(generatereporttext, 'zh-TW', 'gentle_female', True)).start()
            
            # å¼ºåˆ¶é‡æ–°è¿è¡Œä»¥æ˜¾ç¤ºæŠ¥å‘Š
            st.experimental_rerun()
        else:
            st.sidebar.warning("æ²’æœ‰å¯åˆ†æçš„æ•¸æ“šï¼Œè«‹å…ˆåˆ†æç¤¾ç¾¤åª’é«”æˆ–é€²è¡Œå°è©±")


if 'first_visit' not in st.session_state:
    st.session_state.first_visit = True
    # é¦–æ¬¡è®¿é—®æ’­æ”¾æ¬¢è¿è¯­éŸ³
    threading.Thread(target=text_to_speech, args=("æ­¡è¿ä¾†åˆ°å¿ƒéˆå°å¹«æ‰‹ï¼Œå¾ˆé–‹å¿ƒä½ ä¾†æ‰¾æˆ‘èŠå¤©å”·", 'zh-TW', 'gentle_female', True)).start()

# ä¸»æ ‡é¢˜
st.header("å¿ƒéˆå°å¹«æ‰‹")
with st.expander("â„¹ï¸ - é—œæ–¼æˆ‘å€‘", expanded=False):
    st.write(
        """     
-   ç‰ˆæœ¬ï¼šå¿ƒéˆå°å¹«æ‰‹
-   ä½œè€…ï¼šå³ä¸–æ°
	    """
    )

# https://docs.streamlit.io/library/api-reference/performance/st.cache_resource


if 'generated' not in st.session_state:
    st.session_state['generated'] = []

if 'past' not in st.session_state:
    st.session_state['past'] = []

# æ–°å¢: åˆå§‹åŒ–ç”¨æˆ·è¾“å…¥å­˜å‚¨
if 'user_inputs' not in st.session_state:
    st.session_state['user_inputs'] = []

user_col, ensure_col = st.columns([5, 1])

def get_text():
    """å–å¾—ä½¿ç”¨è€…è¼¸å…¥æ–‡å­—çš„å‡½æ•¸"""
    input_text = user_col.text_area("è«‹åœ¨ä¸‹åˆ—æ–‡å­—æ–¹å¡Šè¼¸å…¥æ‚¨çš„è«®è©¢å…§å®¹ï¼š","", key="input", placeholder="è«‹è¼¸å…¥æ‚¨çš„æ±‚åŠ©å…§å®¹ï¼Œä¸¦ä¸”é»æ“Šå‚³é€æŒ‰éˆ•")
    
    # æ£€æŸ¥æ•æ„Ÿè¯ä½†ä¸ä¸­æ–­æµç¨‹
    if any(keyword in input_text for keyword in sensitive_keywords):
        # æ˜¾ç¤ºç´§æ€¥å¸®åŠ©ä¿¡æ¯
        st.warning("""
        ğŸš¨ åµæ¸¬åˆ°æ‚¨å¯èƒ½éœ€è¦ç·Šæ€¥å”åŠ©ï¼š
        é¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222
        ç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000
        æˆ‘å€‘éå¸¸é—œå¿ƒæ‚¨çš„å®‰å…¨ï¼
        """)
        
    if ensure_col.button("å‚³é€", key="send_button", use_container_width=True):
        if input_text:
            return input_text  
    return None  # ç¡®ä¿æ²¡æœ‰ç‚¹å‡»æ—¶è¿”å›None

# ç”¨æˆ·è¾“å…¥å¤„ç†éƒ¨åˆ†
user_input = None  # åˆå§‹åŒ–å˜é‡
user_input = get_text()  # è·å–ç”¨æˆ·è¾“å…¥

if user_input is not None:  # ç¡®ä¿ user_input è¢«å®šä¹‰åå†ä½¿ç”¨
    # æ£€æµ‹äº¤å‹éœ€æ±‚
    if any(phrase in user_input for phrase in friendship_keywords):
        # æ˜¾ç¤ºç¤¾åŒºä¸­å¿ƒèµ„æº
        st.info("""
        ğŸ¤ æˆ‘å€‘æ³¨æ„åˆ°æ‚¨æƒ³èªè­˜æ–°æœ‹å‹ï¼Œä»¥ä¸‹æ˜¯ä¸€äº›ç¤¾å€ä¸­å¿ƒçš„è³‡è¨Šï¼š
        - é¦™æ¸¯é’å¹´å”æœƒï¼šhttps://www.hkfyg.org.hk/
        - æ˜æ„›ç¤¾å€ä¸­å¿ƒï¼šhttps://www.caritas.org.hk/
        - é¦™æ¸¯éŠæ¨‚å ´å”æœƒï¼šhttps://www.hkpa.hk/
        - æ±è¯ä¸‰é™¢ç¤¾å€ä¸­å¿ƒï¼šhttps://www.tungwah.org.hk/
        å¸Œæœ›æ‚¨èƒ½æ‰¾åˆ°å¿—åŒé“åˆçš„æœ‹å‹ï¼
        """)
    
    # æ–°å¢: ä¿å­˜ç”¨æˆ·è¾“å…¥æ—¶æ·»åŠ æ ‡ç­¾
    tags = []
    
    # æ£€æŸ¥æ•æ„Ÿè¯
    if any(keyword in user_input for keyword in sensitive_keywords):
        tags.append('æ•æ„Ÿå†…å®¹')
    
    # æ£€æŸ¥äº¤å‹éœ€æ±‚
    if any(phrase in user_input for phrase in friendship_keywords):
        tags.append('äº¤å‹éœ€æ±‚')
    
    # ä¿å­˜ç”¨æˆ·è¾“å…¥ç”¨äºæƒ…æ„Ÿåˆ†æ
    st.session_state.user_inputs.append({
        'content': user_input,
        'date': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        'tags': tags  # æ–°å¢æ ‡ç­¾å­—æ®µ
    })
    
    st.session_state.past.append(user_input)
    output = answer(st.session_state['past'], st.session_state["generated"])
    st.session_state.generated.append(output)
    
    # åœ¨å›å¤ä¸­å†æ¬¡æ·»åŠ ç´§æ€¥å¸®åŠ©ä¿¡æ¯ï¼ˆå¦‚æœåŒ…å«æ•æ„Ÿè¯ï¼‰
    if any(keyword in user_input for keyword in sensitive_keywords):
        # åœ¨åŸå§‹å›å¤åæ·»åŠ ç´§æ€¥å¸®åŠ©ä¿¡æ¯
        emergency_note = (
            "\n\nâ¤ï¸ æˆ‘å€‘éå¸¸é—œå¿ƒæ‚¨çš„å®‰å…¨ï¼"
            "\nå¦‚æœæ‚¨éœ€è¦ç·Šæ€¥å”åŠ©ï¼Œè«‹ç«‹å³è¯ç¹«ï¼š"
            "\né¦™æ¸¯æ’’ç‘ªåˆ©äºé˜²æ­¢è‡ªæ®ºæœƒï¼š+852 2389 2222"
            "\nç”Ÿå‘½ç†±ç·šï¼š+852 2382 0000"
            "\næ‚¨ä¸¦ä¸å­¤å–®ï¼Œæˆ‘å€‘éƒ½åœ¨é€™è£¡æ”¯æŒæ‚¨ï¼"
        )
        st.session_state.generated[-1] += emergency_note
    
    # å°†å¯¹è¯å†å²ä¿å­˜æˆjsonæ–‡ä»¶
    dialog_history = {
        'user': st.session_state['past'],
        'bot': st.session_state["generated"]
    }
    # ç¡®ä¿ 'id' å·²åˆå§‹åŒ–
    if 'id' in st.session_state:
        with open(os.path.join("./history", str(st.session_state['id'])+'.json'), "w", encoding="utf-8") as f:
            json.dump(dialog_history, f, indent=4, ensure_ascii=False)
    else:
        st.error("æœƒè©±IDæœªåˆå§‹åŒ–ï¼Œç„¡æ³•å„²å­˜å°è©±æ­·å²")
    with open(os.path.join("./history", str(st.session_state['id'])+'.json'), "w", encoding="utf-8") as f:
        json.dump(dialog_history, f, indent=4, ensure_ascii=False)

if 'welcome_played' not in st.session_state:
    st.session_state.welcome_played = False

if (st.session_state['generated'] 
    and not st.session_state.get('show_comprehensive_report', False)
    and not st.session_state.get('form_submitted', False)
    and not st.session_state.get('report_showing', False)
    and not st.session_state.get('report_displayed', False)):
    
    # æ˜¾ç¤ºæ‰€æœ‰å†å²æ¶ˆæ¯
    for i in range(len(st.session_state['generated'])):
        if i == 0 and not st.session_state.welcome_played:
            # é¦–æ¬¡å¯¹è¯æ—¶æ˜¾ç¤ºæ¬¢è¿æ¶ˆæ¯
            message(st.session_state['past'][i], is_user=True, key=str(i) + '_user', avatar_style="avataaars", seed=26)
            message("ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å€‹äººå°ˆå±¬æ•¸ä½è¼”å°å“¡ç”œå¿ƒè€å¸«ï¼Œæ­¡è¿æ‰¾æˆ‘å‚¾è¨´ã€è«‡å¿ƒâ¤ï¸ï¼ŒæœŸå¾…å¹«åŠ©ä½ ï¼", key=str(i), avatar_style="avataaars", seed=5)
            
            # æ’­æ”¾æ¬¢è¿æ¶ˆæ¯å¹¶æ ‡è®°ä¸ºå·²æ’­æ”¾
            if not st.session_state.welcome_played:
                threading.Thread(target=text_to_speech, args=("ä½ å¥½ï¼æˆ‘æ˜¯ä½ çš„å€‹äººå°ˆå±¬æ•¸ä½è¼”å°å“¡ç”œå¿ƒè€å¸«ï¼Œæ­¡è¿æ‰¾æˆ‘å‚¾è¨´ã€è«‡å¿ƒâ¤ï¸ï¼ŒæœŸå¾…å¹«åŠ©ä½ ï¼", 'zh-TW', 'gentle_female', True)).start()
                st.session_state.welcome_played = True
        else:
            # æ˜¾ç¤ºå…¶ä»–å¯¹è¯æ¶ˆæ¯
            message(st.session_state['past'][i], is_user=True, key=str(i) + '_user', avatar_style="avataaars", seed=26)
            message(st.session_state["generated"][i], key=str(i), avatar_style="avataaars", seed=5)
    
    # åªæ’­æ”¾æœ€æ–°çš„å›å¤
    if (len(st.session_state['generated']) > 0 
        and not st.session_state.get('show_comprehensive_report', False)
        and not st.session_state.get('form_submitted', False)
        and not st.session_state.get('report_showing', False)
        and not st.session_state.get('report_displayed', False)):
        
        if not st.session_state.get('skip_audio', False):
            latest_response = st.session_state["generated"][-1]
            threading.Thread(target=text_to_speech, args=(latest_response, 'zh-TW', 'gentle_female', True)).start()
        else:
            # é‡ç½®è·³è¿‡æ ‡å¿—
            st.session_state.skip_audio = False

if st.button("æ¸…ç†å°è©±å¿«å–"):
    st.session_state['generated'] = []
    st.session_state['past'] = []
    st.session_state['user_inputs'] = []
    st.success("å°è©±å¿«å–å·²æ¸…ç†")